{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<font size=\"5\">**Part 3: Prediction of Accident Severity**</font>\n",
        "\n",
        "<font size = \"4\">**Objectives:**</font>\n",
        "\n",
        "•\tDevelop sophisticated predictive models that can accurately forecast the severity of traffic accidents based on a multifaceted set of factors, including location, time of day, weather conditions, and types of vehicles involved. This initiative aims to leverage machine learning algorithms to anticipate the potential impact of accidents.\n",
        "\n",
        "•\tEmploy machine learning techniques to categorize crashes into distinct severity levels. This classification will aid emergency response teams in prioritizing incidents and optimizing resource deployment, ultimately enhancing the efficiency and effectiveness of emergency services.\n"
      ],
      "metadata": {
        "id": "-eW4XwGUzlXy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font size=\"5\">**Install and update the libraries**</font>"
      ],
      "metadata": {
        "id": "uufnozUj1X-4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i94S1rDy4mZY",
        "outputId": "85525e4f-8c06-453c-f5e5-a818081f2ce5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (0.13.1)\n",
            "Collecting seaborn\n",
            "  Downloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.10/dist-packages (from seaborn) (1.25.2)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.10/dist-packages (from seaborn) (2.0.3)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.10/dist-packages (from seaborn) (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2->seaborn) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.16.0)\n",
            "Installing collected packages: seaborn\n",
            "  Attempting uninstall: seaborn\n",
            "    Found existing installation: seaborn 0.13.1\n",
            "    Uninstalling seaborn-0.13.1:\n",
            "      Successfully uninstalled seaborn-0.13.1\n",
            "Successfully installed seaborn-0.13.2\n"
          ]
        }
      ],
      "source": [
        "# Install Java\n",
        "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Download Spark\n",
        "!wget -q https://dlcdn.apache.org/spark/spark-3.5.1/spark-3.5.1-bin-hadoop3.tgz\n",
        "\n",
        "# Unzip the Spark archive\n",
        "!tar xf spark-3.5.1-bin-hadoop3.tgz\n",
        "\n",
        "# Install findspark - a library that makes it easier to locate Spark\n",
        "!pip install -q findspark\n",
        "\n",
        "!pip install --upgrade seaborn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font size=\"5\">**Mount the google drive for data loading**</font>"
      ],
      "metadata": {
        "id": "pzbCkTj511Y5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwfMHftp41Jy",
        "outputId": "6075819d-bb71-4d89-c8f8-a1e94154e779"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "import findspark\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set Java and Spark home based on the installation paths\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.1-bin-hadoop3\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font size=\"5\">**Start the SparkSession and load the data into it**</font>"
      ],
      "metadata": {
        "id": "oLFCw0xz2BaX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oF8bPDJi5Ty5",
        "outputId": "ab86de4e-82ed-4941-f5b9-25a23ef2a483"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- CRASH DATE: string (nullable = true)\n",
            " |-- CRASH TIME: string (nullable = true)\n",
            " |-- BOROUGH: string (nullable = true)\n",
            " |-- ZIP CODE: string (nullable = true)\n",
            " |-- LATITUDE: double (nullable = true)\n",
            " |-- LONGITUDE: double (nullable = true)\n",
            " |-- LOCATION: string (nullable = true)\n",
            " |-- ON STREET NAME: string (nullable = true)\n",
            " |-- CROSS STREET NAME: string (nullable = true)\n",
            " |-- OFF STREET NAME: string (nullable = true)\n",
            " |-- NUMBER OF PERSONS INJURED: string (nullable = true)\n",
            " |-- NUMBER OF PERSONS KILLED: integer (nullable = true)\n",
            " |-- NUMBER OF PEDESTRIANS INJURED: integer (nullable = true)\n",
            " |-- NUMBER OF PEDESTRIANS KILLED: integer (nullable = true)\n",
            " |-- NUMBER OF CYCLIST INJURED: integer (nullable = true)\n",
            " |-- NUMBER OF CYCLIST KILLED: string (nullable = true)\n",
            " |-- NUMBER OF MOTORIST INJURED: string (nullable = true)\n",
            " |-- NUMBER OF MOTORIST KILLED: integer (nullable = true)\n",
            " |-- CONTRIBUTING FACTOR VEHICLE 1: string (nullable = true)\n",
            " |-- CONTRIBUTING FACTOR VEHICLE 2: string (nullable = true)\n",
            " |-- CONTRIBUTING FACTOR VEHICLE 3: string (nullable = true)\n",
            " |-- CONTRIBUTING FACTOR VEHICLE 4: string (nullable = true)\n",
            " |-- CONTRIBUTING FACTOR VEHICLE 5: string (nullable = true)\n",
            " |-- COLLISION_ID: integer (nullable = true)\n",
            " |-- VEHICLE TYPE CODE 1: string (nullable = true)\n",
            " |-- VEHICLE TYPE CODE 2: string (nullable = true)\n",
            " |-- VEHICLE TYPE CODE 3: string (nullable = true)\n",
            " |-- VEHICLE TYPE CODE 4: string (nullable = true)\n",
            " |-- VEHICLE TYPE CODE 5: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Now you can create a SparkSession without the FileNotFoundError\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"NYC Motor Vehicle Collisions Analysis\") \\\n",
        "    .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n",
        "data_path = \"/content/drive/MyDrive/Motor_Vehicle_Collisions_-_Crashes.csv\"\n",
        "df = spark.read.csv(data_path, header=True, inferSchema=True)\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font size=\"5\">**Standardize the features**</font>"
      ],
      "metadata": {
        "id": "izSVmM_t4jV3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "oPdAnHqX58be"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import col, count, when\n",
        "\n",
        "# Standardize the vehicle type\n",
        "replacement_dict = {\n",
        "    \"SPORT UTILITY / STATION WAGON\": \"Station Wagon/Sport Utility Vehicle\",\n",
        "    \"PICK-UP TRUCK\": \"Pick-up Truck\",\n",
        "    \"TAXI\": \"Taxi\",\n",
        "    \"4 dr sedan\": \"Sedan\"\n",
        "}\n",
        "\n",
        "for original_value, new_value in replacement_dict.items():\n",
        "    df = df.withColumn(\"VEHICLE TYPE CODE 1\",\n",
        "                       when(col(\"VEHICLE TYPE CODE 1\") == original_value, new_value)\n",
        "                       .otherwise(col(\"VEHICLE TYPE CODE 1\")))\n",
        "    df = df.withColumn(\"VEHICLE TYPE CODE 2\",\n",
        "                       when(col(\"VEHICLE TYPE CODE 2\") == original_value, new_value)\n",
        "                       .otherwise(col(\"VEHICLE TYPE CODE 2\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_-cgRhWL6SRQ"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql.functions import to_timestamp, concat, lit, col, year, month, hour, date_format\n",
        "\n",
        "\n",
        "# \"CRASH DATE\" is in the format \"MM/dd/yyyy\" and \"CRASH TIME\" is in \"HH:mm\"\n",
        "# Concatenate date and time into a single column with a space separator\n",
        "df = df.withColumn(\"CRASH DATE TIME\", to_timestamp(concat(col(\"CRASH DATE\"), lit(\" \"), col(\"CRASH TIME\")), 'MM/dd/yyyy HH:mm'))\n",
        "\n",
        "# Now proceed to extract components as previously shown\n",
        "df = df.withColumn(\"CRASH YEAR\", year(\"CRASH DATE TIME\"))\n",
        "df = df.withColumn(\"CRASH MONTH\", month(\"CRASH DATE TIME\"))\n",
        "df = df.withColumn(\"CRASH MONTH NAME\", date_format(\"CRASH DATE TIME\", 'MMM'))\n",
        "df = df.withColumn(\"CRASH HOUR\", hour(\"CRASH DATE TIME\"))\n",
        "df = df.withColumn(\"CRASH WEEK\", date_format(\"CRASH DATE TIME\", 'E'))\n",
        "\n",
        "# Show the results to verify\n",
        "# df.select(\"CRASH DATE TIME\", \"CRASH YEAR\", \"CRASH MONTH\", \"CRASH HOUR\", \"CRASH WEEK\").show(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Gaw1XncZ6b2z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85acf289-70f1-4f48-b3bf-ece7c993a398"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+------------+\n",
            "|CRASH MONTH|CRASH SEASON|\n",
            "+-----------+------------+\n",
            "|          9|      Autumn|\n",
            "|          3|      Spring|\n",
            "|          6|      Summer|\n",
            "|          9|      Autumn|\n",
            "|         12|      Winter|\n",
            "|          4|      Spring|\n",
            "|         12|      Winter|\n",
            "|         12|      Winter|\n",
            "|         12|      Winter|\n",
            "|         12|      Winter|\n",
            "|         12|      Winter|\n",
            "|         12|      Winter|\n",
            "|         12|      Winter|\n",
            "|         12|      Winter|\n",
            "|         12|      Winter|\n",
            "|         12|      Winter|\n",
            "|         12|      Winter|\n",
            "|         12|      Winter|\n",
            "|         12|      Winter|\n",
            "|         12|      Winter|\n",
            "+-----------+------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Assuming your Spark session is already created and DataFrame is loaded as `df`\n",
        "df = df.withColumn(\n",
        "    \"CRASH SEASON\",\n",
        "    when((col(\"CRASH MONTH\") >= 3) & (col(\"CRASH MONTH\") <= 5), \"Spring\")\n",
        "    .when((col(\"CRASH MONTH\") >= 6) & (col(\"CRASH MONTH\") <= 8), \"Summer\")\n",
        "    .when((col(\"CRASH MONTH\") >= 9) & (col(\"CRASH MONTH\") <= 11), \"Autumn\")\n",
        "    .otherwise(\"Winter\")\n",
        ")\n",
        "\n",
        "df.select(\"CRASH MONTH\", \"CRASH SEASON\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "sZcR2Mc_6h7b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0c27231-83d3-4596-a7b3-8028ed10155b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-------------+\n",
            "|CRASH HOUR|CRASH DAYTIME|\n",
            "+----------+-------------+\n",
            "|         2|Early Morning|\n",
            "|        11|      Morning|\n",
            "|         6|      Morning|\n",
            "|         9|      Morning|\n",
            "|         8|      Morning|\n",
            "|        12|      Morning|\n",
            "|        17|    Afternoon|\n",
            "|         8|      Morning|\n",
            "|        21|      Evening|\n",
            "|        14|    Afternoon|\n",
            "|         0|Early Morning|\n",
            "|        16|    Afternoon|\n",
            "|         8|      Morning|\n",
            "|         0|Early Morning|\n",
            "|        23|      Evening|\n",
            "|        17|    Afternoon|\n",
            "|        20|      Evening|\n",
            "|         1|Early Morning|\n",
            "|        19|      Evening|\n",
            "|        14|    Afternoon|\n",
            "+----------+-------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Proceed hour into daytime for later onehot-encoding\n",
        "df = df.withColumn(\n",
        "    \"CRASH DAYTIME\",\n",
        "    when((col(\"CRASH HOUR\") >= 6) & (col(\"CRASH HOUR\") < 13), \"Morning\")\n",
        "    .when((col(\"CRASH HOUR\") >= 13) & (col(\"CRASH HOUR\") < 19), \"Afternoon\")\n",
        "    .when((col(\"CRASH HOUR\") >= 19) & (col(\"CRASH HOUR\") <= 23), \"Evening\")\n",
        "    .otherwise(\"Early Morning\")\n",
        ")\n",
        "\n",
        "df.select(\"CRASH HOUR\", \"CRASH DAYTIME\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLI6cufo8Gd1",
        "outputId": "b0bdbc55-8e1d-47b2-cd2f-c197c762666d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- CRASH DAYTIME: string (nullable = false)\n",
            " |-- CRASH WEEK: string (nullable = true)\n",
            " |-- CRASH SEASON: string (nullable = false)\n",
            " |-- LATITUDE: double (nullable = true)\n",
            " |-- LONGITUDE: double (nullable = true)\n",
            " |-- NUMBER OF PERSONS INJURED: string (nullable = true)\n",
            " |-- NUMBER OF PERSONS KILLED: integer (nullable = true)\n",
            " |-- CONTRIBUTING FACTOR VEHICLE 1: string (nullable = true)\n",
            " |-- CONTRIBUTING FACTOR VEHICLE 2: string (nullable = true)\n",
            " |-- VEHICLE TYPE CODE 1: string (nullable = true)\n",
            " |-- VEHICLE TYPE CODE 2: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "retain = [\"CRASH DAYTIME\", \"CRASH WEEK\", \"CRASH SEASON\",\"LATITUDE\", \"LONGITUDE\", \"NUMBER OF PERSONS INJURED\", \"NUMBER OF PERSONS KILLED\",\n",
        "          \"CONTRIBUTING FACTOR VEHICLE 1\", \"CONTRIBUTING FACTOR VEHICLE 2\",\n",
        "          \"VEHICLE TYPE CODE 1\", \"VEHICLE TYPE CODE 2\"]\n",
        "\n",
        "# 选择数据\n",
        "nyc_traffic_collisions_analysis = df.select(*retain)\n",
        "\n",
        "# 删除重复数据\n",
        "nyc_traffic_collisions_analysis = nyc_traffic_collisions_analysis.dropDuplicates()\n",
        "\n",
        "# 使用printSchema()来查看结构，使用describe().show()来查看统计信息，或者使用count()来看行数\n",
        "nyc_traffic_collisions_analysis.printSchema()\n",
        "# nyc_traffic_collisions_analysis.describe().show()\n",
        "# print(\"Total rows:\", nyc_traffic_collisions_analysis.count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "uh0hh6UmDFYG"
      },
      "outputs": [],
      "source": [
        "# nyc_traffic_collisions_analysis.select(\"VEHICLE TYPE CODE 1\").distinct().count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "4Xx72JU7TuRZ"
      },
      "outputs": [],
      "source": [
        "# nyc_traffic_collisions_analysis.select(\"VEHICLE TYPE CODE 2\").distinct().count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "EeHyMP8aTzRx"
      },
      "outputs": [],
      "source": [
        "# nyc_traffic_collisions_analysis.select(\"CONTRIBUTING FACTOR VEHICLE 1\").distinct().count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "iUMeDuvPafF6"
      },
      "outputs": [],
      "source": [
        "# nyc_traffic_collisions_analysis.select(\"CONTRIBUTING FACTOR VEHICLE 2\").distinct().count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "r3OsaD8oUbP2"
      },
      "outputs": [],
      "source": [
        "# Create one more column for evaluating the severity\n",
        "from pyspark.sql.types import IntegerType\n",
        "nyc_traffic_collisions_analysis = nyc_traffic_collisions_analysis.withColumn(\"NUMBER OF PERSONS INJURED\", col(\"NUMBER OF PERSONS INJURED\").cast(IntegerType()))\n",
        "nyc_traffic_collisions_analysis = nyc_traffic_collisions_analysis.withColumn(\n",
        "    \"SEVERITY\",\n",
        "    # if no people injured or killed\n",
        "    when((col(\"NUMBER OF PERSONS INJURED\") == 0) & (col(\"NUMBER OF PERSONS KILLED\") == 0), \"No People Loss\")\n",
        "\n",
        "    # if no people die and only small amount of injured\n",
        "    .when((((col(\"NUMBER OF PERSONS INJURED\") > 0) & (col(\"NUMBER OF PERSONS INJURED\") < 5)) & (col(\"NUMBER OF PERSONS KILLED\") == 0)), \"Light Severity\")\n",
        "\n",
        "    # if people killed or injured amount larger than 5\n",
        "    .when((((10 > col(\"NUMBER OF PERSONS INJURED\")) & (col(\"NUMBER OF PERSONS INJURED\") > 5)) & ((col(\"NUMBER OF PERSONS KILLED\") < 5) & (col(\"NUMBER OF PERSONS KILLED\") >= 0))) | \\\n",
        "     ((col(\"NUMBER OF PERSONS KILLED\") < 5) & (col(\"NUMBER OF PERSONS KILLED\") >= 0)), \"Medium Severity\")\n",
        "\n",
        "    .otherwise(\"Heavy Severity\")\n",
        ")\n",
        "\n",
        "# nyc_traffic_collisions_analysis.select(\"NUMBER OF PERSONS INJURED\", \"NUMBER OF PERSONS KILLED\", \"SEVERITY\").show(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "dxnQkd_eeKJE"
      },
      "outputs": [],
      "source": [
        "# nyc_traffic_collisions_analysis.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font size=\"5\">**Process the data with NA values**</font>\n",
        "\n",
        "* In this part, we need to decide whether the data should be drop or not as some of them just have only one factor, therefore use \"unspecified\" will be better than just drop them.\n",
        "* In contrast, if both two contributing factors and vehicle codes are null(unspecified), drop it."
      ],
      "metadata": {
        "id": "l8er_mOG5FK2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2crTw-m-cIk4",
        "outputId": "c78ce502-adaf-4a45-def3-f5367050413f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1834112"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "# Proceed with NA value\n",
        "\n",
        "# These are accidents that no any info for vehicle, dropped later\n",
        "nyc_traffic_collisions_analysis_NA = nyc_traffic_collisions_analysis.filter((nyc_traffic_collisions_analysis[\"CONTRIBUTING FACTOR VEHICLE 1\"].isNull() == True) \\\n",
        "                                                                         & (nyc_traffic_collisions_analysis[\"CONTRIBUTING FACTOR VEHICLE 2\"].isNull()==True) \\\n",
        "                                                                         & (nyc_traffic_collisions_analysis[\"VEHICLE TYPE CODE 1\"].isNull()==True) & \\\n",
        "                                                                         (nyc_traffic_collisions_analysis[\"VEHICLE TYPE CODE 2\"].isNull()==True))\n",
        "nyc_traffic_collisions_analysis_NA.count()\n",
        "nyc_traffic_collisions_analysis_NAProcessed = nyc_traffic_collisions_analysis.subtract(nyc_traffic_collisions_analysis_NA)\n",
        "nyc_traffic_collisions_analysis_NAProcessed = nyc_traffic_collisions_analysis_NAProcessed.dropna(subset=['LATITUDE','LONGITUDE'])\n",
        "nyc_traffic_collisions_analysis_NAProcessed.count()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "IUuMMhwTh7Yu"
      },
      "outputs": [],
      "source": [
        "# filled NA value for Unspecified\n",
        "nyc_traffic_collisions_analysis_NAProcessed_2 = nyc_traffic_collisions_analysis_NAProcessed.fillna({\"CONTRIBUTING FACTOR VEHICLE 1\": \"Unspecified\",\\\n",
        "                        \"CONTRIBUTING FACTOR VEHICLE 2\": \"Unspecified\",\n",
        "                        \"VEHICLE TYPE CODE 1\": \"Unspecified\",\n",
        "                        \"VEHICLE TYPE CODE 2\": \"Unspecified\"})"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nyc_traffic_collisions_analysis_NAProcessed_2 = nyc_traffic_collisions_analysis_NAProcessed_2.filter((nyc_traffic_collisions_analysis_NAProcessed_2[\"CONTRIBUTING FACTOR VEHICLE 1\"] != \"Unspecified\") \\\n",
        "                                                                         & (nyc_traffic_collisions_analysis_NAProcessed_2[\"CONTRIBUTING FACTOR VEHICLE 2\"] != \"Unspecified\") \\\n",
        "                                                                         & (nyc_traffic_collisions_analysis_NAProcessed_2[\"VEHICLE TYPE CODE 1\"] != \"Unspecified\") & \\\n",
        "                                                                         (nyc_traffic_collisions_analysis_NAProcessed_2[\"VEHICLE TYPE CODE 2\"] != \"Unspecified\"))"
      ],
      "metadata": {
        "id": "R0uQgLlBH2_U"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font size=\"5\">**Assemble the onehot-encoder**</font>\n",
        "* In this part, we need to convert string attributes into the string index, and then use the onehotencoder to assemble them for later learning."
      ],
      "metadata": {
        "id": "RfdAeWSzNZ_X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "ylL6lrg7acyT"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
        "# We need to use one hot to get the season, weekday, daytime, vehicle, and severity\n",
        "# For efficiency, drop the origin one after converted one\n",
        "\n",
        "# CRASH DAYTIME\n",
        "crash_daytime_indexer = StringIndexer(inputCol = \"CRASH DAYTIME\", outputCol = \"CRASH DAYTIME INDEX\").fit(nyc_traffic_collisions_analysis_NAProcessed_2)\n",
        "nyc_traffic_collisions_analysis_NAProcessed_2 = crash_daytime_indexer.transform(nyc_traffic_collisions_analysis_NAProcessed_2)\n",
        "crash_daytime_encoder = OneHotEncoder(inputCol = \"CRASH DAYTIME INDEX\", outputCol = \"CRASH DAYTIME VECTOR\")\n",
        "ohe = crash_daytime_encoder.fit(nyc_traffic_collisions_analysis_NAProcessed_2)\n",
        "nyc_traffic_collisions_analysis_NAProcessed_2 = ohe.transform(nyc_traffic_collisions_analysis_NAProcessed_2)\n",
        "nyc_traffic_collisions_analysis_NAProcessed_2 = nyc_traffic_collisions_analysis_NAProcessed_2.drop(\"CRASH DAYTIME\")\n",
        "\n",
        "# CRASH WEEK\n",
        "crash_week_indexer = StringIndexer(inputCol = \"CRASH WEEK\", outputCol = \"CRASH WEEK INDEX\").fit(nyc_traffic_collisions_analysis_NAProcessed_2)\n",
        "nyc_traffic_collisions_analysis_NAProcessed_2 = crash_week_indexer.transform(nyc_traffic_collisions_analysis_NAProcessed_2)\n",
        "crash_week_encoder = OneHotEncoder(inputCol = \"CRASH WEEK INDEX\", outputCol = \"CRASH WEEK VECTOR\")\n",
        "ohe = crash_week_encoder.fit(nyc_traffic_collisions_analysis_NAProcessed_2)\n",
        "nyc_traffic_collisions_analysis_NAProcessed_2 = ohe.transform(nyc_traffic_collisions_analysis_NAProcessed_2)\n",
        "nyc_traffic_collisions_analysis_NAProcessed_2 = nyc_traffic_collisions_analysis_NAProcessed_2.drop(\"CRASH WEEK\")\n",
        "\n",
        "# CRASH SEASON\n",
        "crash_season_indexer = StringIndexer(inputCol = \"CRASH SEASON\", outputCol = \"CRASH SEASON INDEX\").fit(nyc_traffic_collisions_analysis_NAProcessed_2)\n",
        "nyc_traffic_collisions_analysis_NAProcessed_2 = crash_season_indexer.transform(nyc_traffic_collisions_analysis_NAProcessed_2)\n",
        "crash_season_encoder = OneHotEncoder(inputCol = \"CRASH SEASON INDEX\", outputCol = \"CRASH SEASON VECTOR\")\n",
        "ohe = crash_season_encoder.fit(nyc_traffic_collisions_analysis_NAProcessed_2)\n",
        "nyc_traffic_collisions_analysis_NAProcessed_2 = ohe.transform(nyc_traffic_collisions_analysis_NAProcessed_2)\n",
        "nyc_traffic_collisions_analysis_NAProcessed_2 = nyc_traffic_collisions_analysis_NAProcessed_2.drop(\"CRASH SEASON\")\n",
        "\n",
        "# CONTRIBUTING FACTOR VEHICLE 1\n",
        "cfv1_indexer = StringIndexer(inputCol = \"CONTRIBUTING FACTOR VEHICLE 1\", outputCol = \"CONTRIBUTING FACTOR VEHICLE 1 INDEX\").fit(nyc_traffic_collisions_analysis_NAProcessed_2)\n",
        "nyc_traffic_collisions_analysis_NAProcessed_2 = cfv1_indexer.transform(nyc_traffic_collisions_analysis_NAProcessed_2)\n",
        "cfv1_encoder = OneHotEncoder(inputCol = \"CONTRIBUTING FACTOR VEHICLE 1 INDEX\", outputCol = \"CONTRIBUTING FACTOR VEHICLE 1 VECTOR\")\n",
        "ohe = cfv1_encoder.fit(nyc_traffic_collisions_analysis_NAProcessed_2)\n",
        "nyc_traffic_collisions_analysis_NAProcessed_2 = ohe.transform(nyc_traffic_collisions_analysis_NAProcessed_2)\n",
        "nyc_traffic_collisions_analysis_NAProcessed_2 = nyc_traffic_collisions_analysis_NAProcessed_2.drop(\"CONTRIBUTING FACTOR VEHICLE 1\")\n",
        "\n",
        "# CONTRIBUTING FACTOR VEHICLE 2\n",
        "cfv2_indexer = StringIndexer(inputCol = \"CONTRIBUTING FACTOR VEHICLE 2\", outputCol = \"CONTRIBUTING FACTOR VEHICLE 2 INDEX\").fit(nyc_traffic_collisions_analysis_NAProcessed_2)\n",
        "nyc_traffic_collisions_analysis_NAProcessed_2 = cfv2_indexer.transform(nyc_traffic_collisions_analysis_NAProcessed_2)\n",
        "cfv2_encoder = OneHotEncoder(inputCol = \"CONTRIBUTING FACTOR VEHICLE 2 INDEX\", outputCol = \"CONTRIBUTING FACTOR VEHICLE 2 VECTOR\")\n",
        "ohe = cfv2_encoder.fit(nyc_traffic_collisions_analysis_NAProcessed_2)\n",
        "nyc_traffic_collisions_analysis_NAProcessed_2 = ohe.transform(nyc_traffic_collisions_analysis_NAProcessed_2)\n",
        "nyc_traffic_collisions_analysis_NAProcessed_2 = nyc_traffic_collisions_analysis_NAProcessed_2.drop(\"CONTRIBUTING FACTOR VEHICLE 2\")\n",
        "\n",
        "# VEHICLE TYPE CODE 1\n",
        "vtc1_indexer = StringIndexer(inputCol = \"VEHICLE TYPE CODE 1\", outputCol = \"VEHICLE TYPE CODE 1 INDEX\").fit(nyc_traffic_collisions_analysis_NAProcessed_2)\n",
        "nyc_traffic_collisions_analysis_NAProcessed_2 = vtc1_indexer.transform(nyc_traffic_collisions_analysis_NAProcessed_2)\n",
        "vtc1_encoder = OneHotEncoder(inputCol = \"VEHICLE TYPE CODE 1 INDEX\", outputCol = \"VEHICLE TYPE CODE 1 VECTOR\")\n",
        "ohe = vtc1_encoder.fit(nyc_traffic_collisions_analysis_NAProcessed_2)\n",
        "nyc_traffic_collisions_analysis_NAProcessed_2 = ohe.transform(nyc_traffic_collisions_analysis_NAProcessed_2)\n",
        "nyc_traffic_collisions_analysis_NAProcessed_2 = nyc_traffic_collisions_analysis_NAProcessed_2.drop(\"VEHICLE TYPE CODE 1\")\n",
        "\n",
        "# VEHICLE TYPE CODE 2\n",
        "vtc2_indexer = StringIndexer(inputCol = \"VEHICLE TYPE CODE 2\", outputCol = \"VEHICLE TYPE CODE 2 INDEX\").fit(nyc_traffic_collisions_analysis_NAProcessed_2)\n",
        "nyc_traffic_collisions_analysis_NAProcessed_2 = vtc2_indexer.transform(nyc_traffic_collisions_analysis_NAProcessed_2)\n",
        "vtc2_encoder = OneHotEncoder(inputCol = \"VEHICLE TYPE CODE 2 INDEX\", outputCol = \"VEHICLE TYPE CODE 2 VECTOR\")\n",
        "ohe = vtc2_encoder.fit(nyc_traffic_collisions_analysis_NAProcessed_2)\n",
        "nyc_traffic_collisions_analysis_NAProcessed_2 = ohe.transform(nyc_traffic_collisions_analysis_NAProcessed_2)\n",
        "nyc_traffic_collisions_analysis_NAProcessed_2 = nyc_traffic_collisions_analysis_NAProcessed_2.drop(\"VEHICLE TYPE CODE 2\")\n",
        "\n",
        "# SEVERITY\n",
        "severity_indexer = StringIndexer(inputCol = \"SEVERITY\", outputCol = \"SEVERITY INDEX\").fit(nyc_traffic_collisions_analysis_NAProcessed_2)\n",
        "nyc_traffic_collisions_analysis_NAProcessed_2 = severity_indexer.transform(nyc_traffic_collisions_analysis_NAProcessed_2)\n",
        "severity_encoder = OneHotEncoder(inputCol = \"SEVERITY INDEX\", outputCol = \"SEVERITY VECTOR\")\n",
        "ohe = severity_encoder.fit(nyc_traffic_collisions_analysis_NAProcessed_2)\n",
        "nyc_traffic_collisions_analysis_NAProcessed_2 = ohe.transform(nyc_traffic_collisions_analysis_NAProcessed_2)\n",
        "# nyc_traffic_collisions_analysis_NAProcessed_2 = nyc_traffic_collisions_analysis_NAProcessed_2.drop(\"VEHICLE TYPE CODE 2\")\n",
        "\n",
        "# nyc_traffic_collisions_analysis_NAProcessed_2.show(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font size=\"5\">**Drop unnecessary columns and assemble them into one vector**</font>"
      ],
      "metadata": {
        "id": "73EIzKJsN0ab"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "IhEhuQpC44eo"
      },
      "outputs": [],
      "source": [
        "# Extract the needed ones foe later training\n",
        "needed = [\"CRASH DAYTIME VECTOR\", \"CRASH WEEK VECTOR\", \"CRASH SEASON VECTOR\", \"LATITUDE\", \"LONGITUDE\",\\\n",
        "          \"CONTRIBUTING FACTOR VEHICLE 1 VECTOR\", \"CONTRIBUTING FACTOR VEHICLE 2 VECTOR\", \"VEHICLE TYPE CODE 1 VECTOR\", \"VEHICLE TYPE CODE 2 VECTOR\",\"SEVERITY INDEX\"]\n",
        "nyc_process_data = nyc_traffic_collisions_analysis_NAProcessed_2.select(*needed)\n",
        "# nyc_process_data.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "SibRQCLc60Nq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65599946-242f-4231-e089-5befd7846060"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------+\n",
            "|            features|SEVERITY INDEX|\n",
            "+--------------------+--------------+\n",
            "|(1087,[1,6,9,12,1...|           0.0|\n",
            "|(1087,[1,4,11,12,...|           1.0|\n",
            "|(1087,[0,5,10,12,...|           1.0|\n",
            "|(1087,[1,3,12,13,...|           0.0|\n",
            "|(1087,[1,7,11,12,...|           0.0|\n",
            "|(1087,[1,3,9,12,1...|           0.0|\n",
            "|(1087,[1,8,10,12,...|           0.0|\n",
            "|(1087,[0,4,12,13,...|           1.0|\n",
            "|(1087,[0,4,11,12,...|           1.0|\n",
            "|(1087,[0,6,9,12,1...|           0.0|\n",
            "+--------------------+--------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import IntegerType\n",
        "nyc_assembler = VectorAssembler(inputCols=[\"CRASH DAYTIME VECTOR\", \"CRASH WEEK VECTOR\", \"CRASH SEASON VECTOR\", \"LATITUDE\", \"LONGITUDE\",\\\n",
        "          \"CONTRIBUTING FACTOR VEHICLE 1 VECTOR\", \"CONTRIBUTING FACTOR VEHICLE 2 VECTOR\", \"VEHICLE TYPE CODE 1 VECTOR\", \"VEHICLE TYPE CODE 2 VECTOR\"], outputCol=\"features\")\n",
        "nyc_process_data = nyc_assembler.transform(nyc_process_data)\n",
        "\n",
        "nyc_process_data.select(['features','SEVERITY INDEX']).show(10)\n",
        "# # UDF\n",
        "# get_vector_size = udf(lambda vector: len(vector), IntegerType())\n",
        "# nyc_process_data_with_size = nyc_process_data.withColumn(\"feature_size\", get_vector_size(\"features\"))\n",
        "# nyc_process_data_with_size.select(\"feature_size\").show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "LBYoxMkYA_uX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce75cd1f-b52b-4549-f78d-e75a69fb5a92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "191018\n"
          ]
        }
      ],
      "source": [
        "model_df = nyc_process_data.select(['features', 'SEVERITY INDEX'])\n",
        "train_df, test_df = model_df.randomSplit([0.8, 0.2])\n",
        "print(train_df.count())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<font size=\"5\">**Method: Random Forest**</font>\n",
        "* The reason to choose the random forest is that it has a relatively great performance for the multi-class classification job\n",
        "* After trying different hyperparameters, there is only a little difference for the performance. The key of improving performance is to changing the choose of  the valid features."
      ],
      "metadata": {
        "id": "44mXcvq8OcRp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Niz0hN8bCN_a",
        "outputId": "621d8290-7e92-4844-fce4-5146057db183"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start Process -- numTrees: 50, maxDepths: 6, impurity: gini\n",
            " Result of numTrees: 50, maxDepths: 6, impurity: gini\n",
            "The accuracy of RF on test data is 76%\n",
            "The precision rate on test data is 78%\n",
            "-------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from pyspark.ml.classification import RandomForestClassifier\n",
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "num_Trees = [50]\n",
        "maxDepths = [6]\n",
        "impurities = [\"gini\"]\n",
        "seed=17721487907\n",
        "\n",
        "for num_Tree in num_Trees:\n",
        "  for maxDepth in maxDepths:\n",
        "    for impurity in impurities:\n",
        "      print(f\"Start Process -- numTrees: {num_Tree}, maxDepths: {maxDepth}, impurity: {impurity}\")\n",
        "      rf_classifier = RandomForestClassifier(labelCol='SEVERITY INDEX', maxDepth = maxDepth ,maxMemoryInMB=512, impurity=impurity, numTrees = num_Tree, seed = seed).fit(train_df)\n",
        "\n",
        "      # Test\n",
        "      rf_prediction = rf_classifier.transform(test_df)\n",
        "      rf_accuracy = MulticlassClassificationEvaluator(labelCol = \"SEVERITY INDEX\", metricName = \"accuracy\").evaluate(rf_prediction)\n",
        "      print(f\" Result of numTrees: {num_Tree}, maxDepths: {maxDepth}, impurity: {impurity}\")\n",
        "      print(\"The accuracy of RF on test data is {0:.0%}\".format(rf_accuracy))\n",
        "\n",
        "      rf_precision = MulticlassClassificationEvaluator(labelCol = \"SEVERITY INDEX\", metricName = \"weightedPrecision\").evaluate(rf_prediction)\n",
        "      print(\"The precision rate on test data is {0:.0%}\".format(rf_precision))\n",
        "      print(\"-------------------------------------------------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
        "# from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "\n",
        "# # specify layers for the neural network:\n",
        "# # input layer of size 4 (features), two intermediate of size 10 and 9\n",
        "# # and output of size 4 (classes)\n",
        "# layers = [1087,256,64,4]\n",
        "\n",
        "# # create the trainer and set its parameters\n",
        "# trainer = MultilayerPerceptronClassifier(labelCol=\"SEVERITY INDEX\", maxIter=100, layers=layers, blockSize=128, seed=1234)\n",
        "\n",
        "# # train the model\n",
        "# model = trainer.fit(train_df)\n",
        "\n",
        "# # # compute accuracy on the test set\n",
        "# result = model.transform(test_df)\n",
        "\n",
        "# predictionAndLabels = result.select(\"prediction\", \"SEVERITY INDEX\")\n",
        "# evaluator = MulticlassClassificationEvaluator(labelCol = \"SEVERITY INDEX\",metricName=\"accuracy\")\n",
        "# print(\"Test set accuracy = \" + str(evaluator.evaluate(predictionAndLabels)))"
      ],
      "metadata": {
        "id": "lBU9u3ppL0BS"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from pyspark.ml.classification import GBTClassifier\n",
        "# gbt_classifier = GBTClassifier(featuresCol='features',labelCol='SEVERITY INDEX', seed = seed).fit(train_df)\n",
        "\n",
        "# gbt_prediction = gbt_classifier.transform(test_df)\n",
        "# gbt_accuracy = MulticlassClassificationEvaluator(labelCol = \"SEVERITY INDEX\", metricName = \"accuracy\").evaluate(gbt_prediction)\n",
        "# print(\"The accuracy of RF on test data is {0:.0%}\".format(rf_accuracy))\n",
        "\n",
        "# gbt_precision = MulticlassClassificationEvaluator(labelCol = \"SEVERITY INDEX\", metricName = \"weightedPrecision\").evaluate(gbt_prediction)\n",
        "# print(\"The precision rate on test data is {0:.0%}\".format(rf_precision))\n",
        "# print(\"-------------------------------------------------------------------------\")"
      ],
      "metadata": {
        "id": "G1gtEHn8Jd-l"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "ow0W18QD7n6F"
      },
      "outputs": [],
      "source": [
        "# from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "# rf_prediction = rf_classifier.transform(test_df)\n",
        "# rf_accuracy = MulticlassClassificationEvaluator(labelCol = \"SEVERITY INDEX\", metricName = \"accuracy\").evaluate(rf_prediction)\n",
        "# print(\"The accuracy of RF on test data is {0:.0%}\".format(rf_accuracy))\n",
        "\n",
        "# rf_precision = MulticlassClassificationEvaluator(labelCol = \"SEVERITY INDEX\", metricName = \"weightedPrecision\").evaluate(rf_prediction)\n",
        "# print(\"The precision rate on test data is {0:.0%}\".format(rf_precision))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
